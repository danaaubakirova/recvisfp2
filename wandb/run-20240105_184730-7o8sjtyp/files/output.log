Dataset: waterbirds
Algorithm: ERM
Root dir: data
Split scheme: official
Dataset kwargs: {}
Download: True
Frac: 1.0
Version: None
Unlabeled split: None
Unlabeled version: None
Use unlabeled y: False
Loader kwargs: {'num_workers': 4, 'pin_memory': True}
Unlabeled loader kwargs: {'num_workers': 8, 'pin_memory': True}
Train loader: standard
Uniform over groups: False
Distinct groups: None
N groups per batch: 4
Unlabeled n groups per batch: None
Batch size: 64
Unlabeled batch size: None
Eval loader: standard
Gradient accumulation steps: 1
Subsample: True
Uniform over classes: False
Add num: 10
Add start: 140
Add interval: 10
Uniform after subsample: False
Uniform add: False
Subsample alpha: 0.5
Subsample ref: keep_in_class
Subsample cap: -1
Subsample cap steps: []
Subsample cap milestones: []
Model: dinov2
Model kwargs: {'pretrained': True}
Pretrained model path: None
Load featurizer only: False
Local norm: none
Teacher model path: None
Transform: image_resize_and_center_crop
Additional train transform: weak
Target resolution: (224, 224)
Resize scale: 1.1428571428571428
Max token length: None
Randaugment n: 2
Transform warmup only: None
Loss function: cross_entropy
Loss kwargs: {}
Groupby fields: ['background', 'y']
Group dro step size: None
Algo log metric: accuracy
Process pseudolabels function: None
Val metric: acc_wg
Val metric decreasing: False
N epochs: 300
Optimizer: SGD
Lr: 0.01
Weight decay: 0.01
Max grad norm: None
Optimizer kwargs: {'momentum': 0.9}
Reinit optim: None
Scheduler: MultiStepLR
Scheduler kwargs: {'verbose': True, 'gamma': 0.1}
Scheduler metric split: val
Scheduler metric name: None
Scheduler multistep milestones: [140]
Scheduler multistep gamma: 0.01
Process outputs function: multiclass_logits_to_pred
Evaluate all splits: True
Eval splits: []
Eval only: False
Eval epoch: None
Device: cuda
Seed: 0
Log dir: ./logs/waterbirds_ERM_lr1e-02_wd1e-02_subsample_add10-from140-every10_weak_bs64_seed0
Log every: 50
Save step: None
Save best: True
Save last: True
Save pred: True
No group logging: False
Progress bar: False
Resume: False
Use wandb: True
Wandb api key path: None
Wandb kwargs: {}
Use data parallel: False
Logger: <utils.Logger object at 0x7f2e246c3190>
Add milestones: [140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290]
Train data...
    y =  landbird, background =  land: n = 3498
    y =  landbird, background = water: n = 184
    y = waterbird, background =  land: n = 56
    y = waterbird, background = water: n = 1057
Validation data...
    y =  landbird, background =  land: n = 467
    y =  landbird, background = water: n = 466
    y = waterbird, background =  land: n = 133
    y = waterbird, background = water: n = 133
Test data...
    y =  landbird, background =  land: n = 2255
    y =  landbird, background = water: n = 2255
    y = waterbird, background =  land: n = 642
    y = waterbird, background = water: n = 642
Using cache found in /home/ag/.cache/torch/hub/facebookresearch_dinov2_main
/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Downloading: "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth" to /home/ag/.cache/torch/hub/checkpoints/dinov2_vitl14_pretrain.pth


 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 1.11G/1.13G [00:05<00:00, 223MB/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.13G/1.13G [00:05<00:00, 204MB/s]
Epoch [0]:
Number of training examples: 224
Train:
Traceback (most recent call last):
  File "/bigstorage/dana/PDE/run_expt.py", line 435, in <module>
    main(config)
  File "/bigstorage/dana/PDE/run_expt.py", line 353, in main
    train(
  File "/bigstorage/dana/PDE/train.py", line 237, in train
    run_epoch(algorithm, datasets['train'], general_logger, epoch, config, train=True, unlabeled_dataset=unlabeled_dataset)
  File "/bigstorage/dana/PDE/train.py", line 53, in run_epoch
    batch_results = algorithm.update(labeled_batch, is_epoch_end=(batch_idx==last_batch_idx))
  File "/bigstorage/dana/PDE/algorithms/single_model_algorithm.py", line 143, in update
    results = self.process_batch(batch, unlabeled_batch)
  File "/bigstorage/dana/PDE/algorithms/ERM.py", line 43, in process_batch
    outputs = self.get_model_output(x, y_true)
  File "/bigstorage/dana/PDE/algorithms/single_model_algorithm.py", line 60, in get_model_output
    outputs = self.model(x)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/bigstorage/dana/PDE/models/initializer.py", line 256, in forward
    x = self.transformer(x)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py", line 321, in forward
    ret = self.forward_features(*args, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py", line 257, in forward_features
    x = blk(x)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py", line 254, in forward
    return super().forward(x_or_x_list)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py", line 112, in forward
    x = x + attn_residual_func(x)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py", line 91, in attn_residual_func
    return self.ls1(self.attn(self.norm1(x)))
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py", line 77, in forward
    return super().forward(x)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py", line 63, in forward
    attn = attn.softmax(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 47.54 GiB total capacity; 32.36 GiB already allocated; 53.12 MiB free; 32.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/bigstorage/dana/PDE/run_expt.py", line 435, in <module>
    main(config)
  File "/bigstorage/dana/PDE/run_expt.py", line 353, in main
    train(
  File "/bigstorage/dana/PDE/train.py", line 237, in train
    run_epoch(algorithm, datasets['train'], general_logger, epoch, config, train=True, unlabeled_dataset=unlabeled_dataset)
  File "/bigstorage/dana/PDE/train.py", line 53, in run_epoch
    batch_results = algorithm.update(labeled_batch, is_epoch_end=(batch_idx==last_batch_idx))
  File "/bigstorage/dana/PDE/algorithms/single_model_algorithm.py", line 143, in update
    results = self.process_batch(batch, unlabeled_batch)
  File "/bigstorage/dana/PDE/algorithms/ERM.py", line 43, in process_batch
    outputs = self.get_model_output(x, y_true)
  File "/bigstorage/dana/PDE/algorithms/single_model_algorithm.py", line 60, in get_model_output
    outputs = self.model(x)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/bigstorage/dana/PDE/models/initializer.py", line 256, in forward
    x = self.transformer(x)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py", line 321, in forward
    ret = self.forward_features(*args, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py", line 257, in forward_features
    x = blk(x)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py", line 254, in forward
    return super().forward(x_or_x_list)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py", line 112, in forward
    x = x + attn_residual_func(x)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py", line 91, in attn_residual_func
    return self.ls1(self.attn(self.norm1(x)))
  File "/home/ag/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py", line 77, in forward
    return super().forward(x)
  File "/home/ag/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py", line 63, in forward
    attn = attn.softmax(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 47.54 GiB total capacity; 32.36 GiB already allocated; 53.12 MiB free; 32.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF