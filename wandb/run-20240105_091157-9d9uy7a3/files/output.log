Dataset: waterbirds
Algorithm: groupDRO
Root dir: data
Split scheme: official
Dataset kwargs: {}
Download: True
Frac: 1.0
Version: None
Unlabeled split: None
Unlabeled version: None
Use unlabeled y: False
Loader kwargs: {'num_workers': 4, 'pin_memory': True}
Unlabeled loader kwargs: {'num_workers': 8, 'pin_memory': True}
Train loader: group
Uniform over groups: True
Distinct groups: True
N groups per batch: 4
Unlabeled n groups per batch: None
Batch size: 64
Unlabeled batch size: None
Eval loader: standard
Gradient accumulation steps: 1
Subsample: True
Uniform over classes: False
Add num: 10
Add start: 140
Add interval: 10
Uniform after subsample: False
Uniform add: False
Subsample alpha: 0.5
Subsample ref: keep_in_class
Subsample cap: -1
Subsample cap steps: []
Subsample cap milestones: []
Model: dino
Model kwargs: {'pretrained': True}
Pretrained model path: None
Load featurizer only: False
Local norm: none
Teacher model path: None
Transform: image_resize_and_center_crop
Additional train transform: weak
Target resolution: (224, 224)
Resize scale: 1.1428571428571428
Max token length: None
Randaugment n: None
Transform warmup only: None
Loss function: cross_entropy
Loss kwargs: {}
Groupby fields: ['background', 'y']
Group dro step size: 0.01
Algo log metric: accuracy
Process pseudolabels function: None
Val metric: acc_wg
Val metric decreasing: False
N epochs: 300
Optimizer: SGD
Lr: 0.01
Weight decay: 0.01
Max grad norm: None
Optimizer kwargs: {'momentum': 0.9}
Reinit optim: None
Scheduler: MultiStepLR
Scheduler kwargs: {'verbose': True, 'gamma': 0.1}
Scheduler metric split: val
Scheduler metric name: None
Scheduler multistep milestones: [140]
Scheduler multistep gamma: 0.01
Process outputs function: multiclass_logits_to_pred
Evaluate all splits: True
Eval splits: []
Eval only: False
Eval epoch: None
Device: cuda
Seed: 0
Log dir: ./logs/waterbirds_groupDRO_lr1e-02_wd1e-02_uniform_subsample_add10-from140-every10_weak_bs64_seed0
Log every: 50
Save step: None
Save best: True
Save last: True
Save pred: True
No group logging: False
Progress bar: False
Resume: False
Use wandb: True
Wandb api key path: None
Wandb kwargs: {}
Use data parallel: False
Logger: <utils.Logger object at 0x7fa0b43157e0>
Add milestones: [140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290]
Train data...
    y =  landbird, background =  land: n = 3498
    y =  landbird, background = water: n = 184
    y = waterbird, background =  land: n = 56
    y = waterbird, background = water: n = 1057
Validation data...
    y =  landbird, background =  land: n = 467
    y =  landbird, background = water: n = 466
    y = waterbird, background =  land: n = 133
    y = waterbird, background = water: n = 133
Test data...
    y =  landbird, background =  land: n = 2255
    y =  landbird, background = water: n = 2255
    y = waterbird, background =  land: n = 642
    y = waterbird, background = water: n = 642
Adjusting learning rate of group 0 to 1.0000e-02.
Using cache found in /home/ag/.cache/torch/hub/facebookresearch_dino_main
/home/ag/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ag/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Epoch [0]:
Number of training examples: 224
Train:
Traceback (most recent call last):
  File "/bigstorage/dana/PDE/run_expt.py", line 435, in <module>
    main(config)
  File "/bigstorage/dana/PDE/run_expt.py", line 353, in main
    train(
  File "/bigstorage/dana/PDE/train.py", line 237, in train
    run_epoch(algorithm, datasets['train'], general_logger, epoch, config, train=True, unlabeled_dataset=unlabeled_dataset)
  File "/bigstorage/dana/PDE/train.py", line 47, in run_epoch
    for labeled_batch in batches:
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1333, in _next_data
    return self._process_data(data)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1359, in _process_data
    data.reraise()
  File "/home/ag/.local/lib/python3.10/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 295, in __getitem__
    return self.dataset[self.indices[idx]]
IndexError: index 1923 is out of bounds for axis 0 with size 224
Traceback (most recent call last):
  File "/bigstorage/dana/PDE/run_expt.py", line 435, in <module>
    main(config)
  File "/bigstorage/dana/PDE/run_expt.py", line 353, in main
    train(
  File "/bigstorage/dana/PDE/train.py", line 237, in train
    run_epoch(algorithm, datasets['train'], general_logger, epoch, config, train=True, unlabeled_dataset=unlabeled_dataset)
  File "/bigstorage/dana/PDE/train.py", line 47, in run_epoch
    for labeled_batch in batches:
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1333, in _next_data
    return self._process_data(data)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1359, in _process_data
    data.reraise()
  File "/home/ag/.local/lib/python3.10/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ag/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 295, in __getitem__
    return self.dataset[self.indices[idx]]
IndexError: index 1923 is out of bounds for axis 0 with size 224